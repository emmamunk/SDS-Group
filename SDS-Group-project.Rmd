---
title: "SDS-M2-Group-project"
author: "Emma"
date: "20 sep 2019"
output: pdf_document
---

SDS 2019

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


M1: GroupAssignmentIntroductionWith the individual assignments, you already performed most of the steps in a typical machinelearning pipeline. You imported some data, cleaned it, explored the variables and theirrelationships using summary statistics and visualisations. You also exercised some standardmachine learning preprocessing procedures such as feature scaling and dealing with missingvalues.You practised unsupervised machine learning techniques for dimensionality reduction (e.g.PCA) and clustering (e.g. KNN) to discover latent relationships between features and groupingsof observations. You finally used supervised machine learning for regression and classificationproblems, where you created models to predict an outcome of interest given some input features.Now it is time to bring all these steps together and apply them to a setting that you findinteresting. This should apply the following tasks.●Identify an interesting problem that can be tackled using data science techniques.●Select and obtain relevant data to do so.●Clean and manipulate the data to make it useful for data science techniques.●Carry out an exploratory data analysis to provide intuition into the content of the data,and interesting relationships to be found in it.●Use unsupervised ML techniques to discover latent relationships within the data.●Use supervised ML techniques to create models that predict an outcome of interest.●Document your workflow in a reconstructable manner.●Report your findings in an accessible manner.
Task descriptionData & Problem identificationIn this exercise, you are asked to choose and obtain a dataset you consider interesting andappropriate for the tasks required. Some of you may already have some ideas about interestingdatasets. There are many open datasets available on the internet (e.g. kaggle or individualprojects like Stanford Open Policing or download some of the Datacamp project datasets) - herea recent list of open data repositories for inspiration:https://towardsdatascience.com/top-sources-for-machine-learning-datasets-bb6d0dc3378bIf you instead want to collect your data (e.g. scraping Twitter or other platforms) – we will nothold you back. However, consider the timeframe.The data should fulfill the following minimum requirements:●It should be big enough to be useful for applying data science techniques (rule of thumb:minimum  > 500 observations, > 10 features).●It contains an interesting outcome to be predicted via supervised ML.●It is not completely trivial and clean, and at least requires a minimum amount of cleaning,munging, preprocessing (eg. no toy dataset such as Iris, diamonds, or cars).Analysis pipelineThe analysis to be carried out by you has to contain elements of data manipulation,exploration, unsupervised and supervised ML.Generally, you can combine parts from the individual assignments and use them as a template forthe module assignment. Going beyond that is not required (but for sure appreciated). Below a(rather detailed) checklist to make sure you have all the pieces.❏Definition of a problem statement and a short outline of the implementation❏Description of data acquisition / how it was collected (by you or the publisher of the data)❏Data preparation❏Data cleaning (if needed)❏Recoding (label encoding, dummy creation etc.)❏Merging and wrangling (if needed)❏Exploratory data analysis❏Relevant! summary statistics❏Relevant! visualisations❏Appropriate description (This is important!)❏Feature scaling (if applicable)❏Missing data imputation (if applicable and deemed relevant)
❏Unsupervised ML❏Dimensionality reduction: E.g. PCA, truncated SVD, NMF❏Clustering: E.g. KMeans / Hierarchical clustering, HDBSCAN❏Insights from the analysis used for further descriptive exploration of the data❏Supervised ML❏Train- / Testset preparation❏classification or/and regression problem❏Use of different algorithms (min. 3) compared and ranked according to theirperformance❏using appropriate metrics (k-fold cross-validation)❏you may include hyperparameter tuning (grid-search, adaptive resampling etc.)❏performance evaluation on the test set (scores, performance reports but alsovisuals where useful)Many of the steps are optional. So choose which methods you deem helpful and relevant toexplore your chosen problem.Note: Quality > Quantity. Consider which analysis, summarization, and visualization adds value.Excessive and unselective outputs (e.g. running 20 different models without providing a reasonfor, providing all possibilities of different plots without discussing and evaluating the insightsgained from it) will not be considered helpful but rather distracting.Documentation and DeliverablesYou are asked to hand in two different report formats, namely:1.Functional computational notebook2.Stakeholder reportComputational NotebookThe notebook targets a machine-learning literate audience. Here you can go deeper into thetechnical details and method considerations. Provide thorough documentation of the wholeprocess, the used methods. Describe the intuition behind the selected and used methods, justifychoices made, and interpret results (e.g. Why scaling? Why splitting the data? Why certaintabulations and visualizations? What can be seen from ... ?, How did you select a particularalgorithm? Why did you scale features in one way or another?).Please provide the notebook as a PDF (Knittered from rmd or converted ipynb) with a public linkto a functional Colab version (test it beforehand in incognito/private mode of your browser)Stakeholder ReportThe stakeholder report (simple PDF, no code) summarises the analysis for a non-technicalaudience. Here you don't need to discuss alternative approaches to standardization and alike.
Instead, you should try to explain the analysis and results, emphasizing its meaning andinterpretation. Aim at a length of not more than 5 pages, including tables & visualizations.Finally●Submission deadline is Thursday, 26. September 23:55.●The evaluation seminar will be on Tuesday, 1. October. Technical details regarding thesubmission and evaluation of the assignment will be sent out by the middle of nextweek.●We will send out a doodle to accommodate your schedules as well find proper alignmentfor R vs Python groups.●In case of trouble/issues/questions, please write on Slack. When possible, try to get helpfrom your classmates in the #module1-assignment-clinique channel. We will also take alook at what's happening there.